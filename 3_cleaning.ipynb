{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lipchenko/lipchenko_HSE_student/blob/main/3_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning** (чистка данных) - этап предварительной обработки данных для анализа данных и машинного обучения.\n",
        "\n",
        "**Примеры Data Cleaning:**\n",
        "- удаление избыточных столбцов из табличных данных;\n",
        "- приведение текста к нижнему регистру;\n",
        "- чистка текста от HTML-артефактов\n",
        "\n",
        "Загрузим тренировочный датасет, почистим его и проанализируем результаты."
      ],
      "metadata": {
        "id": "Vl6M3rDU5XhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1: Загрузка данных\n",
        "\n",
        "Загрузим тренировочный датасет и посмотрим на наши данные.\n",
        "\n",
        "Представим, что мы загрузили статью в формате HTML.\n",
        "\n",
        "Скачаем ее и выведем на экран первые 100 символов с помощью слайсинга (среза)."
      ],
      "metadata": {
        "id": "LK_zsNv26Tsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# загружаем тренировочные данные\n",
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/data_cleaning.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbak8zIx8i6_",
        "outputId": "cd152ce7-3b3f-4841-82db-f772c98b345c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-21 12:53:11--  https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/data_cleaning.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2377 (2.3K) [text/plain]\n",
            "Saving to: ‘data_cleaning.txt’\n",
            "\n",
            "\rdata_cleaning.txt     0%[                    ]       0  --.-KB/s               \rdata_cleaning.txt   100%[===================>]   2.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-21 12:53:11 (44.3 MB/s) - ‘data_cleaning.txt’ saved [2377/2377]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# запишем данные в переменную data\n",
        "with open('data_cleaning.txt', 'r') as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "OJ68oc8E8r9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# выведите на экран первые 100 символов с помощью слайсинга\n",
        "print (data [:100])\n",
        "\n",
        "### ваш код здесь ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ9GtP8s86V8",
        "outputId": "26190ede-1318-4912-d9cc-dca51632c3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html>\n",
            "<head>\n",
            "    <title>Sample HTML Document</title>\n",
            "</head>\n",
            "<body>\n",
            "    <h1>Welcome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2: Удаление артефактов\n",
        "\n",
        "В данных много артефактов - HTML-тегов.\n",
        "\n",
        "Удалим HTML-артефакты с помощью регулярных выражений RegEx"
      ],
      "metadata": {
        "id": "XIGQ9xGQ6t1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# пропишем паттерн для поиска HTML-тегов вида <tag> ... </tag>\n",
        "import re   # загрузим библиотеку для обработки регулярных выражений\n",
        "\n",
        "tag_pattern = r'<[^>]+>'    # паттерн для поиска тегов"
      ],
      "metadata": {
        "id": "aQ0ka1oq9uVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используйте функцию `re.sub` (substitution) для чистки данных\n",
        "\n",
        "`re.sub` ищет в строке `string` соответствия RegEx-паттерну `pattern` и меняет найденное на указанную строку `repl`\n",
        "\n",
        "Как используем функцию: `re.sub(pattern, repl, string)`\n",
        "\n",
        "- `pattern` - паттерн RegEx, соответствия которому будет искать функция\n",
        "- `repl` - на что будем менять найденные соответствия\n",
        "- `string` - где будем искать, наш датасет\n",
        "\n",
        "Запишите результат в переменную `clean_text` и выведите на экран с 720-го по 800-ый символ очищенного текста\n",
        "\n",
        "Используйте слайсинг"
      ],
      "metadata": {
        "id": "h8oleHAo_UaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'<[^>]+>'\n",
        "string = data\n",
        "repl = \"\"\n",
        "clean_text = re.sub(pattern, repl, string)\n",
        "print(clean_text[720:800])\n",
        "\n",
        "# Подсказки:\n",
        "# используйте паттерн, записанный в переменную tag_pattern\n",
        "# замените результат на пустую строку \"\"\n",
        "### ваш код здесь: примените re.sub ###\n",
        "### ваш код здесь: выведите результат с 720-го по 800-ый символ ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjqjGi8P-H6O",
        "outputId": "3c92a279-0119-43f8-f55d-e0f2187b4b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It's crucial to handle HTML entities such as &lt;div&gt;, &lt;p&gt;, &amp;, etc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы удалили не все специальные символы HTML\n",
        "\n",
        "Создадим еще один паттерн и повторим процедуру"
      ],
      "metadata": {
        "id": "fZAWGBC7Ajr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbols_pattern = r'&\\w+;'    # паттерн для поиска специальных символов"
      ],
      "metadata": {
        "id": "jIftruIgBuQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используйте `re.sub` для удаления этих символов\n",
        "\n",
        "Теперь функция принимает на вход паттерн `symbols_pattern` и текст `clean_text`\n",
        "\n",
        "Перезапишите переменную `clean_text`\n",
        "\n",
        "Выведите на экран с 720-го по 800-ый символ, чтобы убедиться в том, что чистка прошла успешно"
      ],
      "metadata": {
        "id": "LJ19ehiFB5d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "symbols_pattern = r'&\\w+;'\n",
        "string = clean_text\n",
        "repl = \"\"\n",
        "clean_text = re.sub(symbols_pattern, repl, string)\n",
        "print(clean_text[720:800])\n",
        "\n",
        "### ваш код здесь: выведите результат с 720-го по 800-ый символ ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsX39hWDCLR6",
        "outputId": "a05a8bb1-f1a2-47d5-a299-3229937320fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It's crucial to handle HTML entities such as div, p, , etc. HTML entities are s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В нашем тексте остались двойные пробелы\n",
        "\n",
        "Уберем им с помощью очередного паттерна"
      ],
      "metadata": {
        "id": "mP70Rf3oCKNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'\\s+'\n",
        "string = clean_text\n",
        "repl = \"\"\n",
        "re.sub(pattern,repl,string)\n",
        "clean_text [:100]\n",
        "# выведите на экран первые 100 символов вашего текущего результата\n",
        "# на этот раз не используйте print\n",
        "### ваш код здесь"
      ],
      "metadata": {
        "id": "GMGJDpaEDHx5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7af75360-7497-4856-fac6-72252a2e81b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n    Sample HTML Document\\n\\n\\n    Welcome to Data Cleaning\\n    This is a sample paragraph with HTML '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем паттерн для поиска двойных пробелов\n",
        "\n",
        "Повторите процедуру, перезапишите результат в `clean_text` и выведите первые 100 символов\n",
        "\n",
        "Что мы запишем в переменную `repl`, чтобы не удалить абсолютно все пробелы?"
      ],
      "metadata": {
        "id": "GpgXBzMxDicX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "space_pattern =r'\\s+'\n",
        "string = clean_text\n",
        "repl =\" \"\n",
        "new_text = re.sub(pattern,repl,string)\n",
        "new_text.strip()[:100]\n",
        "\n",
        "### ваш код здесь: примените re.sub ###\n",
        "### ваш код здесь: выведите первые 100 символов, не используя print ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aQ-ZqbEcDcIE",
        "outputId": "4eeb34d6-280c-4a43-8eba-49f801aa7b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sample HTML Document Welcome to Data Cleaning This is a sample paragraph with HTML artifacts such as'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3: Смена регистра\n",
        "\n",
        "Приведем все слова к нижнему регистру с помощью функции `lower`\n",
        "\n",
        "Запишем результат в переменную `text_lower` и выведем на экран последние 100 символов"
      ],
      "metadata": {
        "id": "Hwtj4l2x6bwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_lower = new_text.lower()\n",
        "print(f'{text_lower[-100:]}')\n",
        "### ваш код здесь: выведите первые 100 символов с конца, используйте слайсинг и не забудьте про - ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbmH8JL0EPeg",
        "outputId": "bfe4cbc7-c8e0-48b4-c00f-d07f08d3b560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e learning models, making predictions, or generating insights to support decision-making processes. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 4: Удаление стоп-слов\n",
        "\n",
        "Удалим частотные слова, которые создают шум для решения задач\n",
        "\n",
        "Загрузим список стоп-слов"
      ],
      "metadata": {
        "id": "vkxdtN1Q6mwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/stopwords.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ypsKpKBE6T-",
        "outputId": "0c1d7e0b-5cc7-4997-b88c-a0d66a0bd732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-21 12:53:34--  https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/data/stopwords.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 954 [text/plain]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "\rstopwords.txt         0%[                    ]       0  --.-KB/s               \rstopwords.txt       100%[===================>]     954  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-21 12:53:34 (62.3 MB/s) - ‘stopwords.txt’ saved [954/954]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# запишем данные в переменную stopwords\n",
        "with open('stopwords.txt', 'r') as f:\n",
        "  stopwords = f.read().split()"
      ],
      "metadata": {
        "id": "_c1xfdkgFNcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведите на экран первые 10 стоп-слов"
      ],
      "metadata": {
        "id": "nansznnrFVN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{stopwords[:10]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pctau5NgFYJG",
        "outputId": "fab849ab-2362-4d11-dbc0-d4f166f768c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью `random` мы можем \"вытянуть\" из списка стоп-слов случайное слово"
      ],
      "metadata": {
        "id": "7C4R5y63Fqrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.choice(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8SX-lcCAFjNc",
        "outputId": "2d075d94-fc7d-4a7a-e9c9-3407a5dfdf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'doing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Вытяните\" еще одно случайное слово и запишите его в переменную `random_word`"
      ],
      "metadata": {
        "id": "tPDPQYYTFwsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_word = random.choices(stopwords)\n",
        "print (random_word)\n"
      ],
      "metadata": {
        "id": "oJkDb27LF75t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec149690-9f20-4490-def2-708f62b80f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['until']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверьте, есть ли это слово в `text_lower` с помощью `in`\n",
        "\n",
        "Выведите на экран это слово"
      ],
      "metadata": {
        "id": "erv_tjszGZlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Результат проверки: {str(random_word) in text_lower}')\n",
        "print(f'Случайное слово: {random_word}')\n",
        "\n",
        "\n",
        "### ваш код здесь: вывод на экран текста \"Результат проверки:\" и проверки с in ###\n",
        "### ваш код здесь: вывод текста \"Случайное слово:\" и random_word ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2NJtaSOGBDT",
        "outputId": "a9fad617-889e-4fad-ac9f-89ee4a49ac72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат проверки: False\n",
            "Случайное слово: ['until']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуйте сгенерировать еще несколько слов и проверить их наличие в `text_lower`\n",
        "\n",
        "Для этого запустите повторно две последние ячейки"
      ],
      "metadata": {
        "id": "VBJJHLvrHCCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Вот так будет выглядеть текст после удаления стоп-слов _без_ токенизации\n",
        "Заменятся все аналогичные сочетания знаков\n",
        "Поэтому перед _удалением_ стоп-слов, проведем токенизацию\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gY7oeXTXGW0D",
        "outputId": "03746623-40b7-4082-9a55-ffe57bb54dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nВот так будет выглядеть текст после удаления стоп-слов _без_ токенизации\\nЗаменятся все аналогичные сочетания знаков\\nПоэтому перед _удалением_ стоп-слов, проведем токенизацию\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 5: Токенизация\n",
        "\n",
        "Токенизируем датасет для дальнейшей работы\n",
        "\n",
        "Создадим 2 набора токенов: с сегментацией по предложениям и с сегментацией по словам"
      ],
      "metadata": {
        "id": "Mepvb0Kw7Ncq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создайте переменную `sentences`\n",
        "\n",
        "С помощью `split` разделите текст на предложения (сегменты, разделенные знаком `.`)\n",
        "\n",
        "Выведите на экран первые 10 элементов `sentences`"
      ],
      "metadata": {
        "id": "Me63a7SNLa6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = text_lower.strip().split('.')\n",
        "print (f'{sentences[0:10]}')\n",
        "### ваш код здесь: split для сегментации по знаку `.` ###\n",
        "### вывод на экран первых 10 предложений ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G13fM_27M1t",
        "outputId": "8a5e7585-5354-4df2-9d1f-173bddfc1de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample html document welcome to data cleaning this is a sample paragraph with html artifacts such as bold and italic text', ' data cleaning is an essential process in preparing data for analysis', ' it involves various techniques to clean, transform, and preprocess data', ' in data cleaning, it\\'s important to remove stop words like \"the\", \"and\", \"of\", etc', ' stop words are common words that are often filtered out from text data because they do not carry significant meaning', \" here's another paragraph\", ' sometimes text is in uppercase or lowercase', \" it's important to standardize the text case to ensure consistency in the dataset\", ' this can be achieved by converting all text to lowercase or uppercase', \" it's crucial to handle html entities such as div, p, , etc\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создайте переменную `tokens`\n",
        "\n",
        "С помощью `split` разделите текст `text_lower` на слова\n",
        "\n",
        "Выведите первые 10 элементов"
      ],
      "metadata": {
        "id": "6J_C8cq0L2rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = (text_lower.strip().split())\n",
        "print (f'{tokens[0:10]}')\n",
        "### ваш код здесь: вывод на экран первых 10 слов ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj3dXh576a5A",
        "outputId": "a1a3ff23-285f-4d5d-ea58-d8fabe390f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'html', 'document', 'welcome', 'to', 'data', 'cleaning', 'this', 'is', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Удалим стоп-слова"
      ],
      "metadata": {
        "id": "5Q7FfNK9MJ_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tokens = []\n",
        "\n",
        "for token in tokens:  # итерация по списку токенов tokens\n",
        "  if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов\n",
        "    clean_tokens.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах\n",
        "print (f'{clean_tokens[0:10]}')\n",
        "### ваш код здесь: вывод на экран первых 10 элементов clean_tokens ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMy-_DNJ0HF3",
        "outputId": "0af06b0b-500b-4d53-ec6b-c8ab3735671b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'html', 'document', 'welcome', 'data', 'cleaning', 'sample', 'paragraph', 'html', 'artifacts']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# И еще одно задание...\n",
        "\n",
        "В ячейке ниже вы сможете загрузить еще один текст\n",
        "\n",
        "Используйте свой код и код из задания, чтобы\n",
        "\n",
        "1. удалить артефакты (html-теги, специальные символы и двойные пробелы)\n",
        "\n",
        "2. привести текст к нижнему регистру\n",
        "\n",
        "3. токенизировать текст по предложениям\n",
        "\n",
        "4. токенизировать текст по словам\n",
        "\n",
        "5. удалить стоп-слова"
      ],
      "metadata": {
        "id": "0EYfMpHZNUkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/extracurricular/artefacts.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE6b-OKsNqK_",
        "outputId": "01b07d42-98c5-4f63-f09f-68092dbe812d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-21 12:54:04--  https://raw.githubusercontent.com/vifirsanova/hse-python-course/main/extracurricular/artefacts.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 845 [text/plain]\n",
            "Saving to: ‘artefacts.txt’\n",
            "\n",
            "\rartefacts.txt         0%[                    ]       0  --.-KB/s               \rartefacts.txt       100%[===================>]     845  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-21 12:54:04 (50.8 MB/s) - ‘artefacts.txt’ saved [845/845]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# запишем данные в переменную artefacts\n",
        "with open('artefacts.txt', 'r') as f:\n",
        "  artefacts = f.read()"
      ],
      "metadata": {
        "id": "CT7eVLpHOMTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'<[^>]+>|&\\w+;|\\s+'\n",
        "string = data\n",
        "repl = \" \"\n",
        "clean_text = re.sub(pattern, repl, string)\n",
        "print(clean_text.strip())\n",
        "\n",
        "little_text = clean_text.strip().lower()\n",
        "print (little_text)\n",
        "\n",
        "\n",
        "sentences = little_text.strip().split('.')\n",
        "print (sentences)\n",
        "\n",
        "\n",
        "tokens = new_text.strip().split()\n",
        "print (f'{tokens}')\n",
        "\n",
        "\n",
        "with open('stopwords.txt', 'r') as f:\n",
        "  stopwords = f.read().split()\n",
        "\n",
        "clean_tokens = []\n",
        "\n",
        "for token in tokens:  # итерация по списку токенов tokens\n",
        "  if token not in stopwords:  # проверяем отсутствие токена в списке стоп-слов\n",
        "    clean_tokens.append(token)  # добавляем токен в новый очищенный список токенов, если его нет в стоп-словах\n",
        "print (f'{clean_tokens}')"
      ],
      "metadata": {
        "id": "9g-geUOsORlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7444b8fb-f729-478a-a5dc-47a3579038d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample HTML Document       Welcome to Data Cleaning   This is a sample paragraph with  HTML artifacts  such as  bold  and  italic  text. Data cleaning is an essential process in preparing data for analysis. It involves various techniques to clean, transform, and preprocess data.   In data cleaning, it's important to remove  stop words  like \"the\", \"and\", \"of\", etc. Stop words are common words that are often filtered out from text data because they do not carry significant meaning.   Here's another paragraph. Sometimes text is in  UPPERCASE  or  lowercase . It's important to standardize the text case to ensure consistency in the dataset. This can be achieved by converting all text to lowercase or uppercase.   It's crucial to handle  HTML entities  such as  div ,  p ,  , etc. HTML entities are special characters or symbols that have specific meanings in HTML. They need to be properly handled to avoid issues during data processing.   Data cleaning also involves dealing with missing values. Missing values can occur due to various reasons such as incomplete data collection or data entry errors. It's essential to identify and handle missing values appropriately to avoid bias in the analysis.   Text data often contains noise such as punctuation marks, special characters, and digits. Removing noise from text data is necessary to focus on the meaningful content. Techniques such as regular expressions can be used for noise removal.   Another important aspect of data cleaning is deduplication. Duplicate records in a dataset can skew analysis results and lead to inaccurate conclusions. Identifying and removing duplicate records ensures data integrity and improves the quality of analysis.   After cleaning the data, it's essential to perform exploratory data analysis (EDA) to gain insights and identify patterns. EDA involves visualizing data, computing summary statistics, and exploring relationships between variables.   Once the data is cleaned and analyzed, it can be used for various purposes such as building machine learning models, making predictions, or generating insights to support decision-making processes.\n",
            "sample html document       welcome to data cleaning   this is a sample paragraph with  html artifacts  such as  bold  and  italic  text. data cleaning is an essential process in preparing data for analysis. it involves various techniques to clean, transform, and preprocess data.   in data cleaning, it's important to remove  stop words  like \"the\", \"and\", \"of\", etc. stop words are common words that are often filtered out from text data because they do not carry significant meaning.   here's another paragraph. sometimes text is in  uppercase  or  lowercase . it's important to standardize the text case to ensure consistency in the dataset. this can be achieved by converting all text to lowercase or uppercase.   it's crucial to handle  html entities  such as  div ,  p ,  , etc. html entities are special characters or symbols that have specific meanings in html. they need to be properly handled to avoid issues during data processing.   data cleaning also involves dealing with missing values. missing values can occur due to various reasons such as incomplete data collection or data entry errors. it's essential to identify and handle missing values appropriately to avoid bias in the analysis.   text data often contains noise such as punctuation marks, special characters, and digits. removing noise from text data is necessary to focus on the meaningful content. techniques such as regular expressions can be used for noise removal.   another important aspect of data cleaning is deduplication. duplicate records in a dataset can skew analysis results and lead to inaccurate conclusions. identifying and removing duplicate records ensures data integrity and improves the quality of analysis.   after cleaning the data, it's essential to perform exploratory data analysis (eda) to gain insights and identify patterns. eda involves visualizing data, computing summary statistics, and exploring relationships between variables.   once the data is cleaned and analyzed, it can be used for various purposes such as building machine learning models, making predictions, or generating insights to support decision-making processes.\n",
            "['sample html document       welcome to data cleaning   this is a sample paragraph with  html artifacts  such as  bold  and  italic  text', ' data cleaning is an essential process in preparing data for analysis', ' it involves various techniques to clean, transform, and preprocess data', '   in data cleaning, it\\'s important to remove  stop words  like \"the\", \"and\", \"of\", etc', ' stop words are common words that are often filtered out from text data because they do not carry significant meaning', \"   here's another paragraph\", ' sometimes text is in  uppercase  or  lowercase ', \" it's important to standardize the text case to ensure consistency in the dataset\", ' this can be achieved by converting all text to lowercase or uppercase', \"   it's crucial to handle  html entities  such as  div ,  p ,  , etc\", ' html entities are special characters or symbols that have specific meanings in html', ' they need to be properly handled to avoid issues during data processing', '   data cleaning also involves dealing with missing values', ' missing values can occur due to various reasons such as incomplete data collection or data entry errors', \" it's essential to identify and handle missing values appropriately to avoid bias in the analysis\", '   text data often contains noise such as punctuation marks, special characters, and digits', ' removing noise from text data is necessary to focus on the meaningful content', ' techniques such as regular expressions can be used for noise removal', '   another important aspect of data cleaning is deduplication', ' duplicate records in a dataset can skew analysis results and lead to inaccurate conclusions', ' identifying and removing duplicate records ensures data integrity and improves the quality of analysis', \"   after cleaning the data, it's essential to perform exploratory data analysis (eda) to gain insights and identify patterns\", ' eda involves visualizing data, computing summary statistics, and exploring relationships between variables', '   once the data is cleaned and analyzed, it can be used for various purposes such as building machine learning models, making predictions, or generating insights to support decision-making processes', '']\n",
            "['Sample', 'HTML', 'Document', 'Welcome', 'to', 'Data', 'Cleaning', 'This', 'is', 'a', 'sample', 'paragraph', 'with', 'HTML', 'artifacts', 'such', 'as', 'bold', 'and', 'italic', 'text.', 'Data', 'cleaning', 'is', 'an', 'essential', 'process', 'in', 'preparing', 'data', 'for', 'analysis.', 'It', 'involves', 'various', 'techniques', 'to', 'clean,', 'transform,', 'and', 'preprocess', 'data.', 'In', 'data', 'cleaning,', \"it's\", 'important', 'to', 'remove', 'stop', 'words', 'like', '\"the\",', '\"and\",', '\"of\",', 'etc.', 'Stop', 'words', 'are', 'common', 'words', 'that', 'are', 'often', 'filtered', 'out', 'from', 'text', 'data', 'because', 'they', 'do', 'not', 'carry', 'significant', 'meaning.', \"Here's\", 'another', 'paragraph.', 'Sometimes', 'text', 'is', 'in', 'UPPERCASE', 'or', 'lowercase.', \"It's\", 'important', 'to', 'standardize', 'the', 'text', 'case', 'to', 'ensure', 'consistency', 'in', 'the', 'dataset.', 'This', 'can', 'be', 'achieved', 'by', 'converting', 'all', 'text', 'to', 'lowercase', 'or', 'uppercase.', \"It's\", 'crucial', 'to', 'handle', 'HTML', 'entities', 'such', 'as', 'div,', 'p,', ',', 'etc.', 'HTML', 'entities', 'are', 'special', 'characters', 'or', 'symbols', 'that', 'have', 'specific', 'meanings', 'in', 'HTML.', 'They', 'need', 'to', 'be', 'properly', 'handled', 'to', 'avoid', 'issues', 'during', 'data', 'processing.', 'Data', 'cleaning', 'also', 'involves', 'dealing', 'with', 'missing', 'values.', 'Missing', 'values', 'can', 'occur', 'due', 'to', 'various', 'reasons', 'such', 'as', 'incomplete', 'data', 'collection', 'or', 'data', 'entry', 'errors.', \"It's\", 'essential', 'to', 'identify', 'and', 'handle', 'missing', 'values', 'appropriately', 'to', 'avoid', 'bias', 'in', 'the', 'analysis.', 'Text', 'data', 'often', 'contains', 'noise', 'such', 'as', 'punctuation', 'marks,', 'special', 'characters,', 'and', 'digits.', 'Removing', 'noise', 'from', 'text', 'data', 'is', 'necessary', 'to', 'focus', 'on', 'the', 'meaningful', 'content.', 'Techniques', 'such', 'as', 'regular', 'expressions', 'can', 'be', 'used', 'for', 'noise', 'removal.', 'Another', 'important', 'aspect', 'of', 'data', 'cleaning', 'is', 'deduplication.', 'Duplicate', 'records', 'in', 'a', 'dataset', 'can', 'skew', 'analysis', 'results', 'and', 'lead', 'to', 'inaccurate', 'conclusions.', 'Identifying', 'and', 'removing', 'duplicate', 'records', 'ensures', 'data', 'integrity', 'and', 'improves', 'the', 'quality', 'of', 'analysis.', 'After', 'cleaning', 'the', 'data,', \"it's\", 'essential', 'to', 'perform', 'exploratory', 'data', 'analysis', '(EDA)', 'to', 'gain', 'insights', 'and', 'identify', 'patterns.', 'EDA', 'involves', 'visualizing', 'data,', 'computing', 'summary', 'statistics,', 'and', 'exploring', 'relationships', 'between', 'variables.', 'Once', 'the', 'data', 'is', 'cleaned', 'and', 'analyzed,', 'it', 'can', 'be', 'used', 'for', 'various', 'purposes', 'such', 'as', 'building', 'machine', 'learning', 'models,', 'making', 'predictions,', 'or', 'generating', 'insights', 'to', 'support', 'decision-making', 'processes.']\n",
            "['Sample', 'HTML', 'Document', 'Welcome', 'Data', 'Cleaning', 'This', 'sample', 'paragraph', 'HTML', 'artifacts', 'bold', 'italic', 'text.', 'Data', 'cleaning', 'essential', 'process', 'preparing', 'data', 'analysis.', 'It', 'involves', 'various', 'techniques', 'clean,', 'transform,', 'preprocess', 'data.', 'In', 'data', 'cleaning,', 'important', 'remove', 'stop', 'words', 'like', '\"the\",', '\"and\",', '\"of\",', 'etc.', 'Stop', 'words', 'common', 'words', 'often', 'filtered', 'text', 'data', 'carry', 'significant', 'meaning.', \"Here's\", 'another', 'paragraph.', 'Sometimes', 'text', 'UPPERCASE', 'lowercase.', \"It's\", 'important', 'standardize', 'text', 'case', 'ensure', 'consistency', 'dataset.', 'This', 'can', 'achieved', 'converting', 'text', 'lowercase', 'uppercase.', \"It's\", 'crucial', 'handle', 'HTML', 'entities', 'div,', 'p,', ',', 'etc.', 'HTML', 'entities', 'special', 'characters', 'symbols', 'specific', 'meanings', 'HTML.', 'They', 'need', 'properly', 'handled', 'avoid', 'issues', 'data', 'processing.', 'Data', 'cleaning', 'also', 'involves', 'dealing', 'missing', 'values.', 'Missing', 'values', 'can', 'occur', 'due', 'various', 'reasons', 'incomplete', 'data', 'collection', 'data', 'entry', 'errors.', \"It's\", 'essential', 'identify', 'handle', 'missing', 'values', 'appropriately', 'avoid', 'bias', 'analysis.', 'Text', 'data', 'often', 'contains', 'noise', 'punctuation', 'marks,', 'special', 'characters,', 'digits.', 'Removing', 'noise', 'text', 'data', 'necessary', 'focus', 'meaningful', 'content.', 'Techniques', 'regular', 'expressions', 'can', 'used', 'noise', 'removal.', 'Another', 'important', 'aspect', 'data', 'cleaning', 'deduplication.', 'Duplicate', 'records', 'dataset', 'can', 'skew', 'analysis', 'results', 'lead', 'inaccurate', 'conclusions.', 'Identifying', 'removing', 'duplicate', 'records', 'ensures', 'data', 'integrity', 'improves', 'quality', 'analysis.', 'After', 'cleaning', 'data,', 'essential', 'perform', 'exploratory', 'data', 'analysis', '(EDA)', 'gain', 'insights', 'identify', 'patterns.', 'EDA', 'involves', 'visualizing', 'data,', 'computing', 'summary', 'statistics,', 'exploring', 'relationships', 'variables.', 'Once', 'data', 'cleaned', 'analyzed,', 'can', 'used', 'various', 'purposes', 'building', 'machine', 'learning', 'models,', 'making', 'predictions,', 'generating', 'insights', 'support', 'decision-making', 'processes.']\n"
          ]
        }
      ]
    }
  ]
}